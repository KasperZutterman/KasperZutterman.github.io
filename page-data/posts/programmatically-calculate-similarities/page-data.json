{"componentChunkName":"component---src-templates-post-template-js","path":"/posts/programmatically-calculate-similarities","result":{"data":{"markdownRemark":{"id":"1cd4fe45-530f-58e1-b0f5-3761082cac6d","html":"<p>One of the most fundamental problems in the current era is being able to find and connect similar information. I challenge you to think of one application on the Web that would not benefit from being able to determine similarities between bits of information. Detection of duplicate documents is becoming increasingly important for businesses and data scientist. Near-duplicate pages found on the Web could be plagiarisms, or they could be mirrors with identical information but released on different outlets. </p>\n<p>In this article we’ll explore a range of techniques and algorithms to determine the similarity between items. We will also explain the how and the why, and have a look at some use cases.</p>\n<h2 id=\"how-can-we-define-similarity\" style=\"position:relative;\"><a href=\"#how-can-we-define-similarity\" aria-label=\"how can we define similarity permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>How can we define similarity?</h2>\n<p>We can define similarity as a measure of how much  2 sets, collections or document have in common when we compare them. The more resemblance the 2 documents have to each other, the higher their similarity. To make this definition resemble a concrete value, we will introduce the Jaccard similarity.</p>\n<h3 id=\"jaccard-similarity\" style=\"position:relative;\"><a href=\"#jaccard-similarity\" aria-label=\"jaccard similarity permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Jaccard similarity</h3>\n<p>The notion of how similar sets are is called ”<a href=\"https://en.wikipedia.org/wiki/Jaccard_index\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Jaccard similarity</a>”. The Jaccard similarity of two sets, A and B, can be calculated by the ratio of their intersection size divided by their union size. We shall denote the Jaccard similarity of A and B by:</p>\n<p>J(A,B) = |A ∩ B| / |A ∪ B|</p>\n<p>Using the above formula, we are able to calculate the exact similarity of 2 sets. To make this easy method transferable to other sources of information, we need techniques to represent these sources of data as sets.\nFor example:</p>\n<p>J(A,B) = |A ∩ B| / |A ∪ B| = |2| / |6 + 4 -2| = 2 / 8 = 0.25</p>\n<p><img src=\"/media/jaccard-similarity.png\" alt=\"Two sets with Jaccard similarity 2/8\"></p>\n<h3 id=\"jaccard-distance\" style=\"position:relative;\"><a href=\"#jaccard-distance\" aria-label=\"jaccard distance permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Jaccard distance</h3>\n<p>The Jaccard similarity is a measure of how close sets are, albeit not really a distance measure. To calculate the distance between two sets utilising the Jaccard similarity calculation, we simply take 1 minus the calculated Jaccard similarity. This is called the Jaccard distance, the closer two sets, the higher the Jaccard similarity and thus the lower the Jaccard distance.</p>\n<p>dJ (A,B) = 1 - J(A,B)</p>\n<h3 id=\"how-to-shingle-documents\" style=\"position:relative;\"><a href=\"#how-to-shingle-documents\" aria-label=\"how to shingle documents permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>How to shingle documents</h3>\n<p>The best performing way to represent a document as a set, for the purpose of finding lexical matches, is to construct a set containing short strings of text that appear within the document. By doing so, same pieces of text will always be identified regardless of their position. </p>\n<h3 id=\"k-shingling\" style=\"position:relative;\"><a href=\"#k-shingling\" aria-label=\"k shingling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>k-Shingling</h3>\n<p>A common shingling technique to represent documents as sets  is k-shingling. As a document can simply be seen as a string of characters, we can define a k-shingle of this document as all possible consecutive k length substrings existing in the document. Here you can see an example of a k=2 shingle:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Original: &quot;Thank you for reading this blog&quot;\n\nk=2 shingled: &quot;Thank you&quot; &quot;you for&quot; &quot;for reading&quot; &quot;reading this&quot; &quot;this blog&quot;</code></pre></div>\n<h3 id=\"shingle-size\" style=\"position:relative;\"><a href=\"#shingle-size\" aria-label=\"shingle size permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Shingle size</h3>\n<p>Choosing the best shingle size can be a daunting task, if you take your shingle size to large, you won’t be able to find similarities between documents, only if they are a near perfect match. If you take your shingle size to small, as an extreme example k = 1, you will have a bias because the top 1000 most frequently used words represent around 85% of all word usage. All common used characters will be shared between most of the documents you will want to compare.</p>\n<p>To avoid a bias towards too little or too much abstraction, you should pick your k size large enough that the probability of any given shingle appearing in any given document is low.</p>\n<p>You could also decide to shingle on characters in favor of words, this way you could have a representation that is less defined by words but more by the order of words following each other.</p>\n<h3 id=\"shingle-hashing\" style=\"position:relative;\"><a href=\"#shingle-hashing\" aria-label=\"shingle hashing permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Shingle hashing</h3>\n<p>Instead of using using string representations as shingles directly, you should favor the use of a hash function to determine a hash representing the shingle. This can lead to memory-usage reduction and faster comparison speeds. To select the best string hashing function, please have a look at this overview by Ozan Yigit.</p>\n<h2 id=\"use-cases\" style=\"position:relative;\"><a href=\"#use-cases\" aria-label=\"use cases permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Use cases</h2>\n<h3 id=\"detecting-plagiarism\" style=\"position:relative;\"><a href=\"#detecting-plagiarism\" aria-label=\"detecting plagiarism permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Detecting Plagiarism:</h3>\n<p>Detecting plagiarised content relies on our ability to find textual similarities between content. Plagiarized documents may only contain some parts of the original piece mixed in and scrambled up between his own content, or content from even more sources. Determining whether documents are plagiarized can be a daunting task because a character by character comparison won’t always yield correct results.</p>\n<h3 id=\"same-source-articles\" style=\"position:relative;\"><a href=\"#same-source-articles\" aria-label=\"same source articles permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Same Source Articles</h3>\n<p>When using news aggregators, such as Google News, you are able to find multiple sources and news outlets reporting about the same event. Aggregators are able to cluster articles about the same topic using similarity scores. Reporter also publish articles through the Associated Press, which can then be redistributed by other websites.</p>\n<h2 id=\"conclusions\" style=\"position:relative;\"><a href=\"#conclusions\" aria-label=\"conclusions permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusions</h2>\n<ul>\n<li>Jaccard Similarity: The Jaccard similarity coefficient is a statistic used to measure the similarity between sets. It is calculated by the ratio of the size of the intersection to the size of the union.</li>\n<li>Jaccard Distance: The Jaccard distance is a distance measure between sets, calculated as one minus the Jaccard Similarity</li>\n<li>Shingling: A way of representing text documents as a set of strings. By representing a document by its set of k-shingles, we form a set of k long characters that consecutively appear in the document. By calculating the Jaccard similarity of shingled sets, we are able to measure the textual similarity of  documents.</li>\n</ul>\n<h2 id=\"resources\" style=\"position:relative;\"><a href=\"#resources\" aria-label=\"resources permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Resources</h2>\n<ul>\n<li><a href=\"http://infolab.stanford.edu/~ullman/mmds/ch3.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Mining Massive Datasets Chapter 3 - Anand Rajaraman</a></li>\n<li><a href=\"https://nlp.stanford.edu/IR-book/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Introduction to Information Retrieval - Christopher D. Manning</a></li>\n<li><a href=\"http://ethen8181.github.io/machine-learning/clustering_old/text_similarity/text_similarity.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Text Similarity - Ethen Liu</a></li>\n<li><a href=\"http://www.cse.yorku.ca/~oz/hash.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Hash Functions - Ozan Yigit</a></li>\n<li><a href=\"https://cp-algorithms.com/string/string-hashing.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">String Hashing - E-Maxx Algorithms</a></li>\n</ul>","fields":{"slug":"/posts/programmatically-calculate-similarities","tagSlugs":["/tag/technology/","/tag/similarity/","/tag/web-development/"]},"frontmatter":{"date":"2020-05-17T22:30:00.000Z","description":"In this article we'll explore a range of techniques and algorithms to determine the similarity between items. We will also explain the how and the why, and have a look at some use cases.","tags":["Technology","Similarity","Web Development"],"title":"How to programmatically calculate similarities","socialImage":"/media/spiderman_similarity_meme.jpg"}}},"pageContext":{"slug":"/posts/programmatically-calculate-similarities"}},"staticQueryHashes":["251939775","3942705351","401334301"]}